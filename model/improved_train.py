#!/usr/bin/env python3
"""
–£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º–∏ —Ç–µ—Ö–Ω–∏–∫–∞–º–∏
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
from torchvision import transforms
import pandas as pd
from PIL import Image
from pathlib import Path
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt
import json
import os
import warnings
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
from improved_car_model import ImprovedCarConditionClassifier
from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR
import torchvision.transforms.functional as TF

warnings.filterwarnings('ignore')

class AdvancedCarDataset(Dataset):
    def __init__(self, csv_file, root_dir, transform=None, is_training=True):
        self.data = pd.read_csv(csv_file)
        self.root_dir = Path(root_dir)
        self.transform = transform
        self.is_training = is_training
        
        print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {len(self.data)}")
        print(f"   –ß–∏—Å—Ç—ã–µ: {len(self.data[self.data['cleanliness'] == 'clean'])}")
        print(f"   –ì—Ä—è–∑–Ω—ã–µ: {len(self.data[self.data['cleanliness'] == 'dirty'])}")
        print(f"   –¶–µ–ª—ã–µ: {len(self.data[self.data['damage'] == 'intact'])}")
        print(f"   –ë–∏—Ç—ã–µ: {len(self.data[self.data['damage'] == 'damaged'])}")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        row = self.data.iloc[idx]
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        img_path = self.root_dir / row['image_path']
        try:
            image = Image.open(img_path).convert('RGB')
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {img_path}: {e}")
            # –°–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫—É
            image = Image.new('RGB', (256, 256), color=(128, 128, 128))
        
        if self.transform:
            image = self.transform(image)
        
        # –ú–µ—Ç–∫–∏ (0/1)
        cleanliness_label = 0 if row['cleanliness'] == 'clean' else 1
        damage_label = 0 if row['damage'] == 'intact' else 1
        
        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–µ—Ç–∫–∞ –¥–ª—è –æ–±—â–µ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
        overall_label = cleanliness_label * 2 + damage_label
        
        return {
            'image': image,
            'cleanliness': torch.tensor(cleanliness_label, dtype=torch.long),
            'damage': torch.tensor(damage_label, dtype=torch.long),
            'overall': torch.tensor(overall_label, dtype=torch.long)
        }

class FocalLoss(nn.Module):
    """Focal Loss –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏"""
    def __init__(self, alpha=1, gamma=2, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
        
    def forward(self, inputs, targets):
        ce_loss = torch.nn.functional.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss

class EarlyStopping:
    """Early Stopping –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è"""
    def __init__(self, patience=7, min_delta=0, restore_best_weights=True):
        self.patience = patience
        self.min_delta = min_delta
        self.restore_best_weights = restore_best_weights
        self.best_loss = None
        self.counter = 0
        self.best_weights = None
        
    def __call__(self, val_loss, model):
        if self.best_loss is None:
            self.best_loss = val_loss
            self.save_checkpoint(model)
        elif val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
            self.save_checkpoint(model)
        else:
            self.counter += 1
            
        if self.counter >= self.patience:
            if self.restore_best_weights:
                model.load_state_dict(self.best_weights)
            return True
        return False
    
    def save_checkpoint(self, model):
        self.best_weights = model.state_dict().copy()

def get_advanced_transforms(is_training=True):
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏"""
    if is_training:
        return transforms.Compose([
            transforms.Resize((320, 320)),
            transforms.RandomCrop(256),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(degrees=15),
            transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.3, hue=0.1),
            transforms.RandomAffine(degrees=0, translate=(0.15, 0.15), scale=(0.85, 1.15)),
            transforms.RandomPerspective(distortion_scale=0.2, p=0.3),
            transforms.RandomGrayscale(p=0.1),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            transforms.RandomErasing(p=0.2, scale=(0.02, 0.33), ratio=(0.3, 3.3))
        ])
    else:
        return transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])

def calculate_class_weights(dataset):
    """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏"""
    cleanliness_labels = [dataset[i]['cleanliness'].item() for i in range(len(dataset))]
    damage_labels = [dataset[i]['damage'].item() for i in range(len(dataset))]
    
    # –í–µ—Å–∞ –¥–ª—è —á–∏—Å—Ç–æ—Ç—ã
    clean_count = cleanliness_labels.count(0)
    dirty_count = cleanliness_labels.count(1)
    cleanliness_weights = torch.tensor([dirty_count / clean_count, clean_count / dirty_count], dtype=torch.float)
    
    # –í–µ—Å–∞ –¥–ª—è –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π
    intact_count = damage_labels.count(0)
    damaged_count = damage_labels.count(1)
    damage_weights = torch.tensor([damaged_count / intact_count, intact_count / damaged_count], dtype=torch.float)
    
    return cleanliness_weights, damage_weights

def train_improved_model(epochs=30, batch_size=16, lr=0.001, patience=10):
    """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
    
    print("üöó –£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∞–Ω–∞–ª–∏–∑–∞ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π")
    print("=" * 55)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö
    if not os.path.exists('data/dataset.csv'):
        print("‚ùå –§–∞–π–ª data/dataset.csv –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        print("üí° –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ: python prepare_4folders.py")
        return False
    
    # –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"üîß –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    
    # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
    train_transform = get_advanced_transforms(is_training=True)
    val_transform = get_advanced_transforms(is_training=False)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print("\nüìä –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    
    df = pd.read_csv('data/dataset.csv')
    train_df = df[df['split'] == 'train'].copy()
    val_df = df[df['split'] == 'val'].copy()
    
    print(f"   –û–±—É—á–∞—é—â–∏—Ö: {len(train_df)}")
    print(f"   –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö: {len(val_df)}")
    
    if len(train_df) < 8:
        print("‚ö†Ô∏è –ú–∞–ª–æ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö! –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –º–∏–Ω–∏–º—É–º 50-100 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    
    # –í—Ä–µ–º–µ–Ω–Ω—ã–µ CSV —Ñ–∞–π–ª—ã
    train_df.to_csv('train_temp.csv', index=False)
    val_df.to_csv('val_temp.csv', index=False)
    
    # –î–∞—Ç–∞—Å–µ—Ç—ã
    train_dataset = AdvancedCarDataset('train_temp.csv', 'data', train_transform, is_training=True)
    val_dataset = AdvancedCarDataset('val_temp.csv', 'data', val_transform, is_training=False)
    
    # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤
    cleanliness_weights, damage_weights = calculate_class_weights(train_dataset)
    cleanliness_weights = cleanliness_weights.to(device)
    damage_weights = damage_weights.to(device)
    
    print(f"   –í–µ—Å–∞ —á–∏—Å—Ç–æ—Ç—ã: {cleanliness_weights}")
    print(f"   –í–µ—Å–∞ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π: {damage_weights}")
    
    # DataLoaders
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)
    
    # –ú–æ–¥–µ–ª—å
    print("\nü§ñ –°–æ–∑–¥–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏...")
    model = ImprovedCarConditionClassifier()
    model.to(device)
    
    # –ö—Ä–∏—Ç–µ—Ä–∏–∏ —Å –≤–µ—Å–∞–º–∏ –∫–ª–∞—Å—Å–æ–≤
    cleanliness_criterion = FocalLoss(alpha=1, gamma=2)
    damage_criterion = FocalLoss(alpha=1, gamma=2)
    overall_criterion = nn.CrossEntropyLoss()
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä —Å weight decay
    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)
    
    # Scheduler
    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)
    
    # Early stopping
    early_stopping = EarlyStopping(patience=patience, min_delta=0.001)
    
    # –ò—Å—Ç–æ—Ä–∏—è
    history = {
        'train_loss': [], 'val_loss': [], 'val_acc': [],
        'clean_acc': [], 'damage_acc': [], 'overall_acc': [],
        'lr': []
    }
    best_val_acc = 0.0
    
    print(f"\nüéØ –ù–∞—á–∞–ª–æ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ {epochs} —ç–ø–æ—Ö...")
    print("=" * 55)
    
    for epoch in range(epochs):
        # –û–ë–£–ß–ï–ù–ò–ï
        model.train()
        train_loss = 0.0
        train_batches = 0
        
        print(f"\n–≠–ø–æ—Ö–∞ {epoch+1}/{epochs}")
        pbar = tqdm(train_loader, desc="–û–±—É—á–µ–Ω–∏–µ")
        
        for batch in pbar:
            images = batch['image'].to(device)
            clean_labels = batch['cleanliness'].to(device)
            damage_labels = batch['damage'].to(device)
            overall_labels = batch['overall'].to(device)
            
            optimizer.zero_grad()
            
            # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
            clean_logits, damage_logits, overall_logits = model(images)
            
            # Loss —Å –≤–µ—Å–∞–º–∏
            clean_loss = cleanliness_criterion(clean_logits, clean_labels)
            damage_loss = damage_criterion(damage_logits, damage_labels)
            overall_loss = overall_criterion(overall_logits, overall_labels)
            
            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π loss
            total_loss = clean_loss + damage_loss + 0.5 * overall_loss
            
            # –û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥
            total_loss.backward()
            
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            train_loss += total_loss.item()
            train_batches += 1
            
            pbar.set_postfix({'loss': total_loss.item(), 'lr': optimizer.param_groups[0]['lr']})
        
        # –í–ê–õ–ò–î–ê–¶–ò–Ø
        model.eval()
        val_loss = 0.0
        correct_clean = 0
        correct_damage = 0
        correct_overall = 0
        total = 0
        
        all_clean_preds = []
        all_clean_labels = []
        all_damage_preds = []
        all_damage_labels = []
        
        with torch.no_grad():
            pbar = tqdm(val_loader, desc="–í–∞–ª–∏–¥–∞—Ü–∏—è")
            for batch in pbar:
                images = batch['image'].to(device)
                clean_labels = batch['cleanliness'].to(device)
                damage_labels = batch['damage'].to(device)
                overall_labels = batch['overall'].to(device)
                
                clean_logits, damage_logits, overall_logits = model(images)
                
                # Loss
                clean_loss = cleanliness_criterion(clean_logits, clean_labels)
                damage_loss = damage_criterion(damage_logits, damage_labels)
                overall_loss = overall_criterion(overall_logits, overall_labels)
                total_loss = clean_loss + damage_loss + 0.5 * overall_loss
                val_loss += total_loss.item()
                
                # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
                _, clean_pred = torch.max(clean_logits, 1)
                _, damage_pred = torch.max(damage_logits, 1)
                _, overall_pred = torch.max(overall_logits, 1)
                
                # –¢–æ—á–Ω–æ—Å—Ç—å
                correct_clean += (clean_pred == clean_labels).sum().item()
                correct_damage += (damage_pred == damage_labels).sum().item()
                correct_overall += (overall_pred == overall_labels).sum().item()
                total += images.size(0)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –º–µ—Ç—Ä–∏–∫
                all_clean_preds.extend(clean_pred.cpu().numpy())
                all_clean_labels.extend(clean_labels.cpu().numpy())
                all_damage_preds.extend(damage_pred.cpu().numpy())
                all_damage_labels.extend(damage_labels.cpu().numpy())
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        avg_train_loss = train_loss / train_batches
        avg_val_loss = val_loss / len(val_loader)
        clean_acc = correct_clean / total
        damage_acc = correct_damage / total
        overall_acc = correct_overall / total
        combined_acc = (clean_acc + damage_acc + overall_acc) / 3
        
        # –û–±–Ω–æ–≤–ª—è–µ–º scheduler
        old_lr = optimizer.param_groups[0]['lr']
        scheduler.step(avg_val_loss)
        new_lr = optimizer.param_groups[0]['lr']
        
        if old_lr != new_lr:
            print(f"üìâ Learning rate –∏–∑–º–µ–Ω–µ–Ω: {old_lr:.6f} -> {new_lr:.6f}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏
        history['train_loss'].append(avg_train_loss)
        history['val_loss'].append(avg_val_loss)
        history['val_acc'].append(combined_acc)
        history['clean_acc'].append(clean_acc)
        history['damage_acc'].append(damage_acc)
        history['overall_acc'].append(overall_acc)
        history['lr'].append(optimizer.param_groups[0]['lr'])
        
        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print(f"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}")
        print(f"Clean Acc: {clean_acc:.4f} | Damage Acc: {damage_acc:.4f} | Overall Acc: {overall_acc:.4f}")
        print(f"Combined Acc: {combined_acc:.4f} | LR: {optimizer.param_groups[0]['lr']:.6f}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        if combined_acc > best_val_acc:
            best_val_acc = combined_acc
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'val_acc': combined_acc,
                'clean_acc': clean_acc,
                'damage_acc': damage_acc,
                'overall_acc': overall_acc,
                'history': history
            }, 'best_improved_model.pth')
            print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å! –¢–æ—á–Ω–æ—Å—Ç—å: {combined_acc:.4f}")
        
        # Early stopping
        if early_stopping(avg_val_loss, model):
            print(f"üõë Early stopping –Ω–∞ —ç–ø–æ—Ö–µ {epoch+1}")
            break
    
    # –î–µ—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    print("\nüìä –î–µ—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:")
    print("–ß–∏—Å—Ç–æ—Ç–∞:")
    print(classification_report(all_clean_labels, all_clean_preds, 
                              target_names=['–ß–∏—Å—Ç—ã–π', '–ì—Ä—è–∑–Ω—ã–π']))
    print("–ü–æ–≤—Ä–µ–∂–¥–µ–Ω–∏—è:")
    print(classification_report(all_damage_labels, all_damage_preds, 
                              target_names=['–¶–µ–ª—ã–π', '–ë–∏—Ç—ã–π']))
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
    torch.save({
        'model_state_dict': model.state_dict(),
        'history': history,
        'final_accuracy': best_val_acc,
        'class_weights': {
            'cleanliness': cleanliness_weights.cpu().numpy(),
            'damage': damage_weights.cpu().numpy()
        }
    }, 'final_improved_model.pth')
    
    # –ì—Ä–∞—Ñ–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è
    plot_training_results(history)
    
    # –û—á–∏—Å—Ç–∫–∞
    os.remove('train_temp.csv')
    os.remove('val_temp.csv')
    
    print(f"\nüéâ –£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print(f"‚úÖ –õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {best_val_acc:.4f}")
    print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: best_improved_model.pth")
    print(f"‚úÖ –ì—Ä–∞—Ñ–∏–∫: improved_training_results.png")
    
    return True

def plot_training_results(history):
    """–°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è"""
    plt.figure(figsize=(15, 10))
    
    # Loss
    plt.subplot(2, 3, 1)
    plt.plot(history['train_loss'], label='Train Loss', color='blue')
    plt.plot(history['val_loss'], label='Val Loss', color='red')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    
    # Accuracy
    plt.subplot(2, 3, 2)
    plt.plot(history['val_acc'], label='Combined Acc', color='green')
    plt.plot(history['clean_acc'], label='Clean Acc', color='orange')
    plt.plot(history['damage_acc'], label='Damage Acc', color='purple')
    plt.plot(history['overall_acc'], label='Overall Acc', color='brown')
    plt.title('Validation Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True)
    
    # Learning Rate
    plt.subplot(2, 3, 3)
    plt.plot(history['lr'], label='Learning Rate', color='red')
    plt.title('Learning Rate Schedule')
    plt.xlabel('Epoch')
    plt.ylabel('Learning Rate')
    plt.yscale('log')
    plt.legend()
    plt.grid(True)
    
    # Loss comparison
    plt.subplot(2, 3, 4)
    plt.plot(history['train_loss'], label='Train', alpha=0.7)
    plt.plot(history['val_loss'], label='Val', alpha=0.7)
    plt.fill_between(range(len(history['train_loss'])), 
                     history['train_loss'], alpha=0.3)
    plt.fill_between(range(len(history['val_loss'])), 
                     history['val_loss'], alpha=0.3)
    plt.title('Loss Comparison')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    
    # Accuracy trends
    plt.subplot(2, 3, 5)
    plt.plot(history['clean_acc'], label='Clean', marker='o', markersize=3)
    plt.plot(history['damage_acc'], label='Damage', marker='s', markersize=3)
    plt.plot(history['overall_acc'], label='Overall', marker='^', markersize=3)
    plt.title('Accuracy Trends')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True)
    
    # Final metrics
    plt.subplot(2, 3, 6)
    final_metrics = [
        history['clean_acc'][-1],
        history['damage_acc'][-1], 
        history['overall_acc'][-1],
        history['val_acc'][-1]
    ]
    metric_names = ['Clean', 'Damage', 'Overall', 'Combined']
    bars = plt.bar(metric_names, final_metrics, color=['skyblue', 'lightcoral', 'lightgreen', 'gold'])
    plt.title('Final Metrics')
    plt.ylabel('Accuracy')
    plt.ylim(0, 1)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
    for bar, value in zip(bars, final_metrics):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                f'{value:.3f}', ha='center', va='bottom')
    
    plt.tight_layout()
    plt.savefig('improved_training_results.png', dpi=150, bbox_inches='tight')
    plt.show()

if __name__ == "__main__":
    # –£–ª—É—á—à–µ–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    config = {
        'epochs': 30,        # –ë–æ–ª—å—à–µ —ç–ø–æ—Ö
        'batch_size': 16,    # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π batch size
        'lr': 0.001,         # Learning rate
        'patience': 10       # Early stopping patience
    }
    
    print("‚öôÔ∏è –£–ª—É—á—à–µ–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è:")
    for key, value in config.items():
        print(f"   {key}: {value}")
    
    success = train_improved_model(**config)
    
    if success:
        print(f"\nüöÄ –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:")
        print(f"1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≥—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è: improved_training_results.png")
        print(f"2. –û–±–Ω–æ–≤–∏—Ç–µ app.py –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏")
        print(f"3. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ API: python app.py")
    else:
        print(f"\n‚ùå –û–±—É—á–µ–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–∞–Ω–Ω—ã–µ.")
