#!/usr/bin/env python3
"""
–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ 4 –ø–∞–ø–æ–∫: clean_whole, clean_broken, dirty_whole, dirty_broken
"""

import os
import shutil
from pathlib import Path
import pandas as pd
from PIL import Image
import random


def process_four_folders(source_dir='photos'):
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏–∑ 4 –ø–∞–ø–æ–∫:
    - clean_whole/ -> —á–∏—Å—Ç—ã–µ —Ü–µ–ª—ã–µ
    - clean_broken/ -> —á–∏—Å—Ç—ã–µ —Å–ª–æ–º–∞–Ω–Ω—ã–µ
    - dirty_whole/ -> –≥—Ä—è–∑–Ω—ã–µ —Ü–µ–ª—ã–µ
    - dirty_broken/ -> –≥—Ä—è–∑–Ω—ã–µ —Å–ª–æ–º–∞–Ω–Ω—ã–µ
    """

    print(f"üìÅ –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞–ø–æ–∫ –∏–∑ {source_dir}/")
    print("=" * 50)

    source_path = Path(source_dir)
    if not source_path.exists():
        print(f"‚ùå –ü–∞–ø–∫–∞ {source_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return False

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω—É–∂–Ω—ã—Ö –ø–∞–ø–æ–∫
    required_folders = ['clean_whole', 'clean_broken', 'dirty_whole', 'dirty_broken']
    missing_folders = []

    for folder in required_folders:
        if not (source_path / folder).exists():
            missing_folders.append(folder)

    if missing_folders:
        print(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –ø–∞–ø–∫–∏: {missing_folders}")
        return False

    # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    create_training_structure()

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é –ø–∞–ø–∫—É
    dataset_data = []

    print("\nüîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...")

    # 1. –ü–∞–ø–∫–∞ clean_whole/ -> clean + intact
    clean_whole_folder = source_path / 'clean_whole'
    dataset_data.extend(process_folder(
        clean_whole_folder,
        cleanliness='clean',
        damage='intact',
        category_name='–ß–∏—Å—Ç—ã–µ —Ü–µ–ª—ã–µ'
    ))

    # 2. –ü–∞–ø–∫–∞ clean_broken/ -> clean + damaged
    clean_broken_folder = source_path / 'clean_broken'
    dataset_data.extend(process_folder(
        clean_broken_folder,
        cleanliness='clean',
        damage='damaged',
        category_name='–ß–∏—Å—Ç—ã–µ —Å–ª–æ–º–∞–Ω–Ω—ã–µ'
    ))

    # 3. –ü–∞–ø–∫–∞ dirty_whole/ -> dirty + intact
    dirty_whole_folder = source_path / 'dirty_whole'
    dataset_data.extend(process_folder(
        dirty_whole_folder,
        cleanliness='dirty',
        damage='intact',
        category_name='–ì—Ä—è–∑–Ω—ã–µ —Ü–µ–ª—ã–µ'
    ))

    # 4. –ü–∞–ø–∫–∞ dirty_broken/ -> dirty + damaged
    dirty_broken_folder = source_path / 'dirty_broken'
    dataset_data.extend(process_folder(
        dirty_broken_folder,
        cleanliness='dirty',
        damage='damaged',
        category_name='–ì—Ä—è–∑–Ω—ã–µ —Å–ª–æ–º–∞–Ω–Ω—ã–µ'
    ))

    if not dataset_data:
        print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
        return False

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º dataset.csv
    df = pd.DataFrame(dataset_data)
    df.to_csv('data/dataset.csv', index=False)

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print_statistics(df)

    return True


def create_training_structure():
    """–°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""

    directories = [
        'data/train/clean_intact',
        'data/train/clean_damaged',
        'data/train/dirty_intact',
        'data/train/dirty_damaged',
        'data/val/clean_intact',
        'data/val/clean_damaged',
        'data/val/dirty_intact',
        'data/val/dirty_damaged'
    ]

    for directory in directories:
        Path(directory).mkdir(parents=True, exist_ok=True)


def process_folder(folder_path, cleanliness, damage, category_name):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–π –ø–∞–ø–∫–∏"""

    print(f"\nüìÇ {category_name}: {folder_path}")

    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.webp'}
    image_files = []

    # –ù–∞—Ö–æ–¥–∏–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    for file_path in folder_path.iterdir():
        if file_path.suffix.lower() in image_extensions and file_path.is_file():
            image_files.append(file_path)

    if not image_files:
        print(f"   ‚ö†Ô∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {folder_path}")
        return []

    print(f"   üì∏ –ù–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(image_files)}")

    dataset_entries = []
    copied_count = 0

    for img_path in image_files:
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ñ–∞–π–ª –º–æ–∂–Ω–æ –æ—Ç–∫—Ä—ã—Ç—å
            with Image.open(img_path) as img:
                img.verify()

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º train/val (80/20)
            split = 'train' if random.random() < 0.8 else 'val'

            # –¶–µ–ª–µ–≤–∞—è –ø–∞–ø–∫–∞
            target_category = f"{cleanliness}_{damage}"
            target_dir = Path(f"data/{split}/{target_category}")
            target_path = target_dir / img_path.name

            # –ö–æ–ø–∏—Ä—É–µ–º —Ñ–∞–π–ª
            shutil.copy2(img_path, target_path)

            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –¥–∞–Ω–Ω—ã–µ
            dataset_entries.append({
                'image_path': f"{split}/{target_category}/{img_path.name}",
                'cleanliness': cleanliness,
                'damage': damage,
                'split': split,
                'original_path': str(img_path)
            })

            copied_count += 1

        except Exception as e:
            print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å {img_path.name}: {e}")
            continue

    print(f"   ‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {copied_count} –∏–∑ {len(image_files)}")
    return dataset_entries


def print_statistics(df):
    """–í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –¥–∞—Ç–∞—Å–µ—Ç—É"""

    print("\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –î–ê–¢–ê–°–ï–¢–ê")
    print("=" * 30)
    print(f"–í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(df)}")
    print(f"–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(df[df['split'] == 'train'])}")
    print(f"–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(df[df['split'] == 'val'])}")

    print("\n–ü–æ —á–∏—Å—Ç–æ—Ç–µ:")
    clean_count = len(df[df['cleanliness'] == 'clean'])
    dirty_count = len(df[df['cleanliness'] == 'dirty'])
    print(f"  –ß–∏—Å—Ç—ã–µ: {clean_count} ({clean_count / len(df) * 100:.1f}%)")
    print(f"  –ì—Ä—è–∑–Ω—ã–µ: {dirty_count} ({dirty_count / len(df) * 100:.1f}%)")

    print("\n–ü–æ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏—è–º:")
    intact_count = len(df[df['damage'] == 'intact'])
    damaged_count = len(df[df['damage'] == 'damaged'])
    print(f"  –¶–µ–ª—ã–µ: {intact_count} ({intact_count / len(df) * 100:.1f}%)")
    print(f"  –°–ª–æ–º–∞–Ω–Ω—ã–µ: {damaged_count} ({damaged_count / len(df) * 100:.1f}%)")

    print("\n–ü–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:")
    categories = df.groupby(['cleanliness', 'damage']).size()
    for (cleanliness, damage), count in categories.items():
        clean_ru = "–ß–∏—Å—Ç—ã–µ" if cleanliness == 'clean' else "–ì—Ä—è–∑–Ω—ã–µ"
        damage_ru = "—Ü–µ–ª—ã–µ" if damage == 'intact' else "—Å–ª–æ–º–∞–Ω–Ω—ã–µ"
        print(f"  {clean_ru} {damage_ru}: {count}")

    print(f"\n‚úÖ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: data/dataset.csv")
    print("üöÄ –¢–µ–ø–µ—Ä—å –º–æ–∂–µ—Ç–µ –∑–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ: python train.py")


def main():
    print("üöó inDrive Car Analysis - –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏–∑ 4 –ø–∞–ø–æ–∫")
    print("=" * 55)

    # –ò—â–µ–º –ø–∞–ø–∫—É —Å –¥–∞–Ω–Ω—ã–º–∏
    possible_dirs = ['photos', 'images', 'data_raw', '.']

    source_dir = None
    for dir_name in possible_dirs:
        dir_path = Path(dir_name)
        if (dir_path.exists() and
                (dir_path / 'clean_whole').exists() and
                (dir_path / 'clean_broken').exists() and
                (dir_path / 'dirty_whole').exists() and
                (dir_path / 'dirty_broken').exists()):
            source_dir = dir_name
            break

    if not source_dir:
        print("üîç –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –Ω–µ —É–¥–∞–ª—Å—è")
        source_dir = input("–í–≤–µ–¥–∏—Ç–µ –ø—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –ø–æ–¥–ø–∞–ø–∫–∞–º–∏ clean_whole/, clean_broken/, dirty_whole/, dirty_broken/: ")
        if not source_dir:
            source_dir = 'photos'

    print(f"üìÅ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–∞–ø–∫–∞: {source_dir}")

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    success = process_four_folders(source_dir)

    if success:
        print("\nüéâ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        print("\nüìã –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:")
        print("1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ data/dataset.csv")
        print("2. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –æ–±—É—á–µ–Ω–∏–µ: python train.py")
        print("3. –ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—Å—è –≤ best_model.pth")
        print("4. –û–±–Ω–æ–≤–∏—Ç–µ app.py —á—Ç–æ–±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å")
    else:
        print("\n‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö")


if __name__ == "__main__":
    main()
