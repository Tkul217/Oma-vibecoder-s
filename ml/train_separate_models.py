#!/usr/bin/env python3
"""
–û–±—É—á–µ–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —á–∏—Å—Ç–æ—Ç—ã –∏ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π
–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö:
data/
‚îú‚îÄ‚îÄ clean/      # –ß–∏—Å—Ç—ã–µ –º–∞—à–∏–Ω—ã
‚îú‚îÄ‚îÄ dirty/      # –ì—Ä—è–∑–Ω—ã–µ –º–∞—à–∏–Ω—ã
‚îú‚îÄ‚îÄ damaged/    # –ë–∏—Ç—ã–µ –º–∞—à–∏–Ω—ã
‚îî‚îÄ‚îÄ intact/     # –¶–µ–ª—ã–µ –º–∞—à–∏–Ω—ã
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import torchvision.transforms as transforms
from torchvision import models
from PIL import Image
import os
import json
from tqdm import tqdm
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict
import ssl

# –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ SSL –ø—Ä–æ–±–ª–µ–º—ã –Ω–∞ macOS
try:
    _create_unverified_https_context = ssl._create_unverified_context
except AttributeError:
    pass
else:
    ssl._create_default_https_context = _create_unverified_https_context

class CarDataset(Dataset):
    """–î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π"""
    
    def __init__(self, image_paths, labels, transform=None):
        self.image_paths = image_paths
        self.labels = labels
        self.transform = transform
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        image_path = self.image_paths[idx]
        label = self.labels[idx]
        
        try:
            image = Image.open(image_path).convert('RGB')
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {image_path}: {e}")
            image = Image.new('RGB', (224, 224), color='black')
        
        if self.transform:
            image = self.transform(image)
        
        return image, label

class CarConditionModel(nn.Module):
    """–ú–æ–¥–µ–ª—å –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
    
    def __init__(self, num_classes=2, pretrained=True):
        super(CarConditionModel, self).__init__()
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º ResNet50 –∫–∞–∫ backbone (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–æ –¥–ª—è –Ω–æ–≤—ã—Ö –≤–µ—Ä—Å–∏–π)
        if pretrained:
            self.backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)
        else:
            self.backbone = models.resnet50(weights=None)
        
        # –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º —Ä–∞–Ω–Ω–∏–µ —Å–ª–æ–∏ –¥–ª—è transfer learning
        for param in list(self.backbone.parameters())[:-30]:
            param.requires_grad = False
        
        # –ó–∞–º–µ–Ω—è–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
        num_features = self.backbone.fc.in_features
        self.backbone.fc = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(num_features, 1024),
            nn.ReLU(),
            nn.BatchNorm1d(1024),
            nn.Dropout(0.3),
            nn.Linear(1024, 512),
            nn.ReLU(),
            nn.BatchNorm1d(512),
            nn.Dropout(0.2),
            nn.Linear(512, num_classes)
        )
    
    def forward(self, x):
        return self.backbone(x)

def load_cleanliness_data(data_dir):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏ —á–∏—Å—Ç–æ—Ç—ã"""
    
    image_paths = []
    labels = []
    class_counts = defaultdict(int)
    
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏ —á–∏—Å—Ç–æ—Ç—ã...")
    
    # Clean = 0, Dirty = 1
    folders = {
        'clean': 0,
        'dirty': 1
    }
    
    valid_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff'}
    
    for folder_name, class_id in folders.items():
        folder_path = os.path.join('../data', folder_name)  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω –ø—É—Ç—å
        
        if not os.path.exists(folder_path):
            print(f"‚ö†Ô∏è  –ü–∞–ø–∫–∞ {folder_path} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
            continue
        
        for filename in os.listdir(folder_path):
            if any(filename.lower().endswith(ext) for ext in valid_extensions):
                image_path = os.path.join(folder_path, filename)
                image_paths.append(image_path)
                labels.append(class_id)
                class_counts[folder_name] += 1
    
    print("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö (—á–∏—Å—Ç–æ—Ç–∞):")
    for folder_name, count in class_counts.items():
        print(f"  {folder_name}: {count} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    print(f"  –í—Å–µ–≥–æ: {len(image_paths)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    
    return image_paths, labels

def load_condition_data(data_dir):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π"""
    
    image_paths = []
    labels = []
    class_counts = defaultdict(int)
    
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π...")
    
    # Intact = 0, Damaged = 1
    folders = {
        'intact': 0,
        'damaged': 1
    }
    
    valid_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff'}
    
    for folder_name, class_id in folders.items():
        folder_path = os.path.join('../data', folder_name)  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω –ø—É—Ç—å
        
        if not os.path.exists(folder_path):
            print(f"‚ö†Ô∏è  –ü–∞–ø–∫–∞ {folder_path} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
            continue
        
        for filename in os.listdir(folder_path):
            if any(filename.lower().endswith(ext) for ext in valid_extensions):
                image_path = os.path.join(folder_path, filename)
                image_paths.append(image_path)
                labels.append(class_id)
                class_counts[folder_name] += 1
    
    print("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö (–ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏—è):")
    for folder_name, count in class_counts.items():
        print(f"  {folder_name}: {count} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    print(f"  –í—Å–µ–≥–æ: {len(image_paths)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    
    return image_paths, labels

def get_transforms():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
    
    train_transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.RandomCrop(224),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(degrees=15),
        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),
        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    
    val_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    
    return train_transform, val_transform

def train_model(model, train_loader, val_loader, model_name, num_epochs=30, device='cuda'):
    """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
    
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=5, factor=0.5)
    
    best_acc = 0.0
    train_losses = []
    val_accuracies = []
    
    print(f"\nüöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ {model_name}")
    print(f"–≠–ø–æ—Ö: {num_epochs}")
    print("-" * 60)
    
    for epoch in range(num_epochs):
        print(f'\n–≠–ø–æ—Ö–∞ {epoch+1}/{num_epochs}')
        
        # –§–∞–∑–∞ –æ–±—É—á–µ–Ω–∏—è
        model.train()
        running_loss = 0.0
        running_corrects = 0
        total_samples = 0
        
        train_bar = tqdm(train_loader, desc="–û–±—É—á–µ–Ω–∏–µ")
        for inputs, labels in train_bar:
            inputs = inputs.to(device)
            labels = labels.to(device)
            
            optimizer.zero_grad()
            
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            
            loss.backward()
            optimizer.step()
            
            _, preds = torch.max(outputs, 1)
            running_loss += loss.item() * inputs.size(0)
            running_corrects += torch.sum(preds == labels.data)
            total_samples += inputs.size(0)
            
            current_acc = running_corrects.double() / total_samples
            train_bar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'acc': f'{current_acc:.4f}'
            })
        
        epoch_loss = running_loss / len(train_loader.dataset)
        epoch_acc = running_corrects.double() / len(train_loader.dataset)
        train_losses.append(epoch_loss)
        
        print(f'–û–±—É—á–µ–Ω–∏–µ - Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')
        
        # –§–∞–∑–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        model.eval()
        val_corrects = 0
        val_total = 0
        all_preds = []
        all_labels = []
        
        with torch.no_grad():
            val_bar = tqdm(val_loader, desc="–í–∞–ª–∏–¥–∞—Ü–∏—è")
            for inputs, labels in val_bar:
                inputs = inputs.to(device)
                labels = labels.to(device)
                
                outputs = model(inputs)
                _, preds = torch.max(outputs, 1)
                
                val_corrects += torch.sum(preds == labels.data)
                val_total += labels.size(0)
                
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
        
        val_acc = val_corrects.double() / val_total
        val_accuracies.append(val_acc)
        
        print(f'–í–∞–ª–∏–¥–∞—Ü–∏—è - Acc: {val_acc:.4f}')
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        if val_acc > best_acc:
            best_acc = val_acc
            torch.save(model.state_dict(), f'models/{model_name}_model.pth')
            print(f'‚úÖ –ù–æ–≤–∞—è –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞! –¢–æ—á–Ω–æ—Å—Ç—å: {best_acc:.4f}')
        
        scheduler.step(val_acc)
    
    print(f"\nüéâ –û–±—É—á–µ–Ω–∏–µ {model_name} –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print(f"–õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {best_acc:.4f}")
    
    return model, best_acc

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è"""
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏
    DATA_DIR = 'unused'  # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –ø—É—Ç–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω—ã –≤ —Ñ—É–Ω–∫—Ü–∏—è—Ö
    BATCH_SIZE = 32
    NUM_EPOCHS = 30
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –º–æ–¥–µ–ª–µ–π
    os.makedirs('models', exist_ok=True)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ CUDA
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"üñ•Ô∏è  –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π
    train_transform, val_transform = get_transforms()
    
    # === –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –ß–ò–°–¢–û–¢–´ ===
    print("\n" + "="*60)
    print("üßΩ –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –ß–ò–°–¢–û–¢–´")
    print("="*60)
    
    try:
        clean_paths, clean_labels = load_cleanliness_data(DATA_DIR)
        
        if len(clean_paths) > 0:
            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
            train_paths, val_paths, train_labels, val_labels = train_test_split(
                clean_paths, clean_labels, test_size=0.2, random_state=42, stratify=clean_labels
            )
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
            train_dataset = CarDataset(train_paths, train_labels, train_transform)
            val_dataset = CarDataset(val_paths, val_labels, val_transform)
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–≥—Ä—É–∑—á–∏–∫–æ–≤
            train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
            val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
            cleanliness_model = CarConditionModel(num_classes=2).to(device)
            cleanliness_model, clean_acc = train_model(
                cleanliness_model, train_loader, val_loader, 
                'cleanliness', NUM_EPOCHS, device
            )
        else:
            print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏ —á–∏—Å—Ç–æ—Ç—ã!")
            clean_acc = 0
    
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏ —á–∏—Å—Ç–æ—Ç—ã: {e}")
        clean_acc = 0
    
    # === –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –ü–û–í–†–ï–ñ–î–ï–ù–ò–ô ===
    print("\n" + "="*60)
    print("üîß –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –ü–û–í–†–ï–ñ–î–ï–ù–ò–ô")
    print("="*60)
    
    try:
        condition_paths, condition_labels = load_condition_data(DATA_DIR)
        
        if len(condition_paths) > 0:
            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
            train_paths, val_paths, train_labels, val_labels = train_test_split(
                condition_paths, condition_labels, test_size=0.2, random_state=42, stratify=condition_labels
            )
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
            train_dataset = CarDataset(train_paths, train_labels, train_transform)
            val_dataset = CarDataset(val_paths, val_labels, val_transform)
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–≥—Ä—É–∑—á–∏–∫–æ–≤
            train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
            val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
            condition_model = CarConditionModel(num_classes=2).to(device)
            condition_model, condition_acc = train_model(
                condition_model, train_loader, val_loader, 
                'condition', NUM_EPOCHS, device
            )
        else:
            print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π!")
            condition_acc = 0
    
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π: {e}")
        condition_acc = 0
    
    # === –ò–¢–û–ì–ò ===
    print("\n" + "="*60)
    print("üéâ –ò–¢–û–ì–ò –û–ë–£–ß–ï–ù–ò–Ø")
    print("="*60)
    print(f"üßΩ –ú–æ–¥–µ–ª—å —á–∏—Å—Ç–æ—Ç—ã: {clean_acc:.4f}")
    print(f"üîß –ú–æ–¥–µ–ª—å –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π: {condition_acc:.4f}")
    print(f"üìÅ –ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–µ: models/")
    print(f"üîÆ –î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ: python analyze.py <–ø—É—Ç—å_–∫_–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é>")

if __name__ == "__main__":
    main()